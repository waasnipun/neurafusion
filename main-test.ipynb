{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mini_imagenet_dataset import MiniImageNetDataset\n",
    "from tools import getDataset, print_class_distribution\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T20:49:31.066661Z",
     "start_time": "2023-12-08T20:49:30.193666Z"
    }
   },
   "id": "7cb51636c35ff179"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n",
    "print('Device:', device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T20:49:31.131235Z",
     "start_time": "2023-12-08T20:49:31.127636Z"
    }
   },
   "id": "d0c1117e99dc7cc0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "root_dir = os.path.join(os.getcwd(), 'datasets/miniImageNet')\n",
    "dataset, label_mapping = getDataset(root_dir, shuffle_images=True)\n",
    "\n",
    "train_dataset = MiniImageNetDataset(dataset=dataset, path=root_dir, phase='train', shuffle_images=True, transform=None)\n",
    "val_dataset = MiniImageNetDataset(dataset=dataset, path=root_dir, phase='val', shuffle_images=True, transform=None)\n",
    "test_dataset = MiniImageNetDataset(dataset=dataset, path=root_dir, phase='test', shuffle_images=True, transform=None)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T20:49:31.548512Z",
     "start_time": "2023-12-08T20:49:31.132719Z"
    }
   },
   "id": "13168c9532e86475"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 4\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "root_dir = os.path.join(os.getcwd(), 'datasets/miniImageNet')\n",
    "dataset, label_mapping = getDataset(root_dir, shuffle_images=True)\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((84, 84)),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((84, 84)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "       \n",
    "       \n",
    "train_dataset = MiniImageNetDataset(dataset=dataset, path=root_dir, phase='train', shuffle_images=True, transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "val_dataset = MiniImageNetDataset(dataset=dataset, path=root_dir, phase='val', shuffle_images=True, transform=transforms)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "test_dataset = MiniImageNetDataset(dataset=dataset, path=root_dir, phase='test', shuffle_images=True, transform=transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T20:49:31.684904Z",
     "start_time": "2023-12-08T20:49:31.553813Z"
    }
   },
   "id": "ca9bdb650d1ebe5e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def eval(net, data_loader, criterion=nn.CrossEntropyLoss()):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        net = net.cuda()\n",
    "    net.eval()\n",
    "    correct = 0.0\n",
    "    num_images = 0.0\n",
    "    loss = 0.0\n",
    "    for i_batch, (images, labels) in enumerate(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outs = net(images)\n",
    "        loss += criterion(outs, labels).item()\n",
    "        _, predicted = torch.max(outs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        num_images += len(labels)\n",
    "        print('testing/evaluating -> batch: %d correct: %d numb images: %d' % (i_batch, correct, num_images) + '\\r', end='')\n",
    "    acc = correct / num_images\n",
    "    loss /= len(data_loader)\n",
    "    return acc, loss\n",
    "\n",
    "\n",
    "# training function\n",
    "def train(net, train_loader, valid_loader):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(params= net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        net = net.cuda()\n",
    "\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        correct = 0.0  # used to accumulate number of correctly recognized images\n",
    "        num_images = 0.0  # used to accumulate number of images\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i_batch, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output_train = net(images)\n",
    "            loss = criterion(output_train, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicts = output_train.argmax(dim=1)\n",
    "            correct += predicts.eq(labels).sum().item()\n",
    "            num_images += len(labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            print('training -> epoch: %d, batch: %d, loss: %f' % (epoch, i_batch, loss.item()) + '\\r', end='')\n",
    "\n",
    "        print()\n",
    "        acc = correct / num_images\n",
    "        acc_eval, val_loss = eval(net, valid_loader)\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        training_losses.append(average_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print('\\nepoch: %d, lr: %f, accuracy: %f, loss: %f, valid accuracy: %f\\n' % (epoch, optimizer.param_groups[0]['lr'], acc, average_loss, acc_eval))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return net, training_losses, val_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T20:49:31.696316Z",
     "start_time": "2023-12-08T20:49:31.691779Z"
    }
   },
   "id": "cfab17de52b40940"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "Batch Size: 64\n",
      "Learning Rate: 0.001\n",
      "Number of Epochs: 10\n",
      "Number of Workers: 4\n",
      "training -> epoch: 0, batch: 562, loss: 3.668384\r\n",
      "testing/evaluating -> batch: 187 correct: 771 numb images: 12000\r\n",
      "epoch: 0, lr: 0.010000, accuracy: 0.081139, loss: 4.100140, valid accuracy: 0.064250\n",
      "training -> epoch: 1, batch: 562, loss: 3.058294\r\n",
      "testing/evaluating -> batch: 187 correct: 2423 numb images: 12000\r\n",
      "epoch: 1, lr: 0.010000, accuracy: 0.180028, loss: 3.434090, valid accuracy: 0.201917\n",
      "training -> epoch: 2, batch: 562, loss: 2.623161\r\n",
      "testing/evaluating -> batch: 187 correct: 2950 numb images: 12000\r\n",
      "epoch: 2, lr: 0.010000, accuracy: 0.253139, loss: 3.028945, valid accuracy: 0.245833\n",
      "training -> epoch: 3, batch: 562, loss: 2.758866\r\n",
      "testing/evaluating -> batch: 187 correct: 2937 numb images: 12000\r\n",
      "epoch: 3, lr: 0.010000, accuracy: 0.310528, loss: 2.735857, valid accuracy: 0.244750\n",
      "training -> epoch: 4, batch: 562, loss: 2.827518\r\n",
      "testing/evaluating -> batch: 187 correct: 2679 numb images: 12000\r\n",
      "epoch: 4, lr: 0.010000, accuracy: 0.365083, loss: 2.477605, valid accuracy: 0.223250\n",
      "training -> epoch: 5, batch: 562, loss: 2.014691\r\n",
      "testing/evaluating -> batch: 187 correct: 4721 numb images: 12000\r\n",
      "epoch: 5, lr: 0.001000, accuracy: 0.503389, loss: 1.912354, valid accuracy: 0.393417\n",
      "training -> epoch: 6, batch: 562, loss: 1.387625\r\n",
      "testing/evaluating -> batch: 187 correct: 4717 numb images: 12000\r\n",
      "epoch: 6, lr: 0.001000, accuracy: 0.539861, loss: 1.762873, valid accuracy: 0.393083\n",
      "training -> epoch: 7, batch: 562, loss: 1.883357\r\n",
      "testing/evaluating -> batch: 187 correct: 4706 numb images: 12000\r\n",
      "epoch: 7, lr: 0.001000, accuracy: 0.561194, loss: 1.667046, valid accuracy: 0.392167\n",
      "training -> epoch: 8, batch: 562, loss: 2.086780\r\n",
      "testing/evaluating -> batch: 187 correct: 4867 numb images: 12000\r\n",
      "epoch: 8, lr: 0.001000, accuracy: 0.589167, loss: 1.574332, valid accuracy: 0.405583\n",
      "training -> epoch: 9, batch: 562, loss: 1.500328\r\n",
      "testing/evaluating -> batch: 187 correct: 4743 numb images: 12000\r\n",
      "epoch: 9, lr: 0.001000, accuracy: 0.608028, loss: 1.488116, valid accuracy: 0.395250\n",
      "testing/evaluating -> batch: 187 correct: 4744 numb images: 12000\r\n",
      "accuracy on testing data: 0.395333\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models.resnet18 import ResNet18\n",
    "\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Number of Epochs: {num_epochs}\")\n",
    "print(f\"Number of Workers: {num_workers}\\n\")\n",
    "\n",
    "# print_class_distribution(train_dataset, \"Training\", label_mapping)\n",
    "# print_class_distribution(val_dataset, \"Validation\", label_mapping)\n",
    "# print_class_distribution(test_dataset, \"Testing\", label_mapping)\n",
    "\n",
    "model = ResNet18(num_classes=100).to(device)\n",
    "model, training_losses, val_losses = train(net=model, train_loader=train_loader, valid_loader=validation_loader)\n",
    "\n",
    "acc_test, test_loss = eval(model, test_loader)\n",
    "print('\\naccuracy on testing data: %f' % acc_test)\n",
    "\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "torch.save(model, os.path.join(os.getcwd(), 'pretrained/resnet18_model_full_2.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T00:53:51.862621Z",
     "start_time": "2023-12-08T00:28:18.503060Z"
    }
   },
   "id": "1111f6cab32449a5"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([68, 48, 78, 90, 45, 39, 15, 27, 91, 81, 83, 30,  2, 39, 58,  8,  7, 11,\n",
      "        33, 17, 98, 50, 60, 76, 74,  7, 45, 64, 33, 52, 45, 61, 31, 55, 74, 67,\n",
      "        17, 17, 71, 21, 22, 46, 25,  9, 81, 69, 81, 89, 22, 21, 68, 91, 56, 71,\n",
      "         1, 91, 50,  9, 43, 57, 39, 75, 56, 17], device='mps:0')\n",
      "tensor([ 5, 30,  3, 90, 45, 39, 77, 79, 53, 83, 97, 36,  2, 39, 97, 84,  7, 89,\n",
      "        33, 17, 98, 66, 60, 38, 74,  7, 64, 30, 54, 52, 45, 68, 31, 57,  3, 67,\n",
      "        27, 17, 26, 62, 54, 97, 76,  9, 81, 58, 36,  2, 10, 71, 25, 40, 56,  1,\n",
      "         1, 91, 50,  9, 64, 57, 47, 75, 37, 17], device='mps:0')\n",
      "27 64\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "\n",
    "for i_batch, (images, labels) in enumerate(test_loader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outs = model(images)\n",
    "    _, predicted = torch.max(outs.data, 1)\n",
    "    print(predicted)\n",
    "    print(labels)\n",
    "    _, predicted = torch.max(outs.data, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    print(correct, len(labels))\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T00:59:27.037819Z",
     "start_time": "2023-12-08T00:59:16.296902Z"
    }
   },
   "id": "e9d6f8e94e82f5e9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "Batch Size: 64\n",
      "Learning Rate: 0.01\n",
      "Number of Epochs: 10\n",
      "Number of Workers: 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.76 GB, other allocations: 1.96 GB, max allowed: 9.07 GB). Tried to allocate 381.57 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/q_/vt2y0l_534b26099_6gg5hl40000gn/T/ipykernel_10141/2366922604.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mViT\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m84\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_classes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m512\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdepth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheads\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmlp_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1024\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdropout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memb_dropout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining_losses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_losses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_loader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidation_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0macc_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/q_/vt2y0l_534b26099_6gg5hl40000gn/T/ipykernel_10141/2623270769.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(net, train_loader, valid_loader)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimages\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0moutput_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/pytorch_/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/pytorch_/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DL_Olu/project/neurafusion/models/ViT.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    121\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'mean'\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/pytorch_/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/pytorch_/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DL_Olu/project/neurafusion/models/ViT.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     77\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mattn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mff\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mff\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/pytorch_/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/pytorch_/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DL_Olu/project/neurafusion/models/ViT.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0mq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrearrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'b n (h d) -> b h n d'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mqkv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m         \u001B[0mdots\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m         \u001B[0mattn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mattend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdots\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 6.76 GB, other allocations: 1.96 GB, max allowed: 9.07 GB). Tried to allocate 381.57 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from models.ViT import ViT\n",
    "\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Number of Epochs: {num_epochs}\")\n",
    "print(f\"Number of Workers: {num_workers}\\n\")\n",
    "\n",
    "# print_class_distribution(train_dataset, \"Training\", label_mapping)\n",
    "# print_class_distribution(val_dataset, \"Validation\", label_mapping)\n",
    "# print_class_distribution(test_dataset, \"Testing\", label_mapping)\n",
    "\n",
    "model = ViT(image_size=84, patch_size=4, num_classes=100, dim=512, depth=6, heads=8, mlp_dim=1024, dropout=0.2, emb_dropout=0.1).to(device)\n",
    "model, training_losses, val_losses = train(net=model, train_loader=train_loader, valid_loader=validation_loader)\n",
    "\n",
    "acc_test, test_loss = eval(model, test_loader)\n",
    "print('\\naccuracy on testing data: %f' % acc_test)\n",
    "\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "torch.save(model, os.path.join(os.getcwd(), 'pretrained/vit_model_full_2.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T20:50:42.603195Z",
     "start_time": "2023-12-08T20:49:40.896904Z"
    }
   },
   "id": "5946d8482020b135"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "print(f\"Number of Epochs: {num_epochs}\")\n",
    "print(f\"Number of Workers: {num_workers}\\n\")\n",
    "\n",
    "# print_class_distribution(train_dataset, \"Training\", label_mapping)\n",
    "# print_class_distribution(val_dataset, \"Validation\", label_mapping)\n",
    "# print_class_distribution(test_dataset, \"Testing\", label_mapping)\n",
    "\n",
    "model = models.vgg11(weights=None, num_classes=100).to(device)\n",
    "model, training_losses, val_losses = train(net=model, train_loader=train_loader, valid_loader=validation_loader)\n",
    "\n",
    "acc_test, test_loss = eval(model, test_loader)\n",
    "print('\\naccuracy on testing data: %f' % acc_test)\n",
    "\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "torch.save(model, os.path.join(os.getcwd(), 'pretrained/vgg_model_full_2.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-07T23:48:59.323580Z"
    }
   },
   "id": "100a47afbca7d92e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 5 required positional arguments: 'patch_size', 'embed_dim', 'depths', 'num_heads', and 'window_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/q_/vt2y0l_534b26099_6gg5hl40000gn/T/ipykernel_5759/2588802200.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mswin_transformer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSwinTransformer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_classes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining_losses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_losses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_loader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidation_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() missing 5 required positional arguments: 'patch_size', 'embed_dim', 'depths', 'num_heads', and 'window_size'"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.swin_transformer.SwinTransformer(num_classes=100, num_heads=8, window_size=7).to(device)\n",
    "model, training_losses, val_losses = train(net=model, train_loader=train_loader, valid_loader=validation_loader)\n",
    "\n",
    "torch.save(model, os.path.join(os.getcwd(), 'pretrained/swintransformer_model_full_2.pth'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T15:59:07.076419Z",
     "start_time": "2023-12-08T15:59:06.947518Z"
    }
   },
   "id": "6386a7c012c6f29b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = models.vision_transformer.VisionTransformer(num_classes=100, num_heads=8, image_size=84, patch_size=4, mlp_dim=1024, hidden_dim=512, num_layers=6, dropout=0.2).to(device)\n",
    "model, training_losses, val_losses = train(net=model, train_loader=train_loader, valid_loader=validation_loader)\n",
    "\n",
    "acc_test, test_loss = eval(model, test_loader)\n",
    "print('\\naccuracy on testing data: %f' % acc_test)\n",
    "\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "torch.save(model, os.path.join(os.getcwd(), 'pretrained/vgg_model_full_2.pth'))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-08T16:10:09.511634Z"
    }
   },
   "id": "1ea4a865692f9c48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b04fba26407c92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch_",
   "language": "python",
   "display_name": "pytorch_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
